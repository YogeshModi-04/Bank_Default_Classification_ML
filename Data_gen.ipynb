{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the Bank Classification Dataset\n",
    "\n",
    "## Overview\n",
    "This document provides a detailed explanation of the synthetic dataset generated for a bank-related classification problem. The dataset is designed to simulate real-world scenarios where a bank predicts customer behavior, such as loan default or creditworthiness. It includes a mix of numerical and categorical features, ensuring that all columns are meaningful and relevant to the banking domain.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Details\n",
    "### **Rows and Columns**\n",
    "- **Number of Rows:** 1,000,000\n",
    "- **Number of Columns:** 25 (excluding the target variable)\n",
    "\n",
    "### **Target Variable**\n",
    "- **Name:** `Target`\n",
    "- **Type:** Categorical\n",
    "- **Values:**\n",
    "  - `Default`: Represents customers who default on their loans.\n",
    "  - `No Default`: Represents customers who do not default.\n",
    "\n",
    "### **Numerical Features**\n",
    "1. **Age**: Customer's age in years (range: 18-75).\n",
    "2. **Annual_Income**: Customer's annual income in USD, following a normal distribution centered around $50,000 with a standard deviation of $15,000.\n",
    "3. **Loan_Amount**: The amount of the loan issued in USD, normally distributed around $20,000 with a standard deviation of $10,000.\n",
    "4. **Savings_Balance**: Customer's current savings account balance in USD, normally distributed around $10,000 with a standard deviation of $5,000.\n",
    "5. **Years_as_Customer**: Number of years the customer has been with the bank (range: 1-30).\n",
    "6. **Credit_Score**: Customer's credit score (range: 300-850).\n",
    "7. **Debt_to_Income_Ratio**: Debt-to-income ratio, a derived feature (range: 0.0-1.0).\n",
    "8. **Credit_Utilization**: Ratio of credit used to the total available credit (range: 0.0-1.0).\n",
    "9. **Number_of_Credit_Accounts**: Number of credit accounts held by the customer (range: 1-15).\n",
    "10. **Loan_Term_in_Years**: Duration of the loan in years (range: 1-30).\n",
    "11. **Monthly_Installment**: Monthly installment payment in USD, derived based on loan amount and term.\n",
    "12. **Overdraft_Amount**: Overdraft amount used by the customer in USD (mean: $5,000, standard deviation: $2,000).\n",
    "13. **Annual_Expenditure**: Customer's annual expenditure in USD (mean: $40,000, standard deviation: $12,000).\n",
    "\n",
    "### **Categorical Features**\n",
    "1. **Gender**: Customer's gender (“Male”, “Female”, “Non-Binary”).\n",
    "2. **Marital_Status**: Customer's marital status (“Single”, “Married”, “Divorced”, “Widowed”).\n",
    "3. **Employment_Status**: Employment status (“Employed”, “Unemployed”, “Self-Employed”, “Student”, “Retired”).\n",
    "4. **Education_Level**: Highest level of education completed (“High School”, “Bachelor’s”, “Master’s”, “Doctorate”).\n",
    "5. **Loan_Purpose**: Purpose of the loan (“Car”, “Home”, “Education”, “Personal”, “Business”).\n",
    "6. **Has_Credit_Card**: Indicates whether the customer owns a credit card (“Yes”, “No”).\n",
    "7. **Is_Homeowner**: Indicates whether the customer owns a home (“Yes”, “No”).\n",
    "8. **Account_Type**: Type of account held by the customer (“Savings”, “Checking”, “Business”).\n",
    "9. **Customer_Segment**: Categorizes customers into segments (“Premium”, “Regular”, “Occasional”).\n",
    "10. **Preferred_Contact_Channel**: Preferred method of contact (“Email”, “Phone”, “In-Person”).\n",
    "\n",
    "---\n",
    "\n",
    "## Code Explanation\n",
    "### **Numerical Features Generation**\n",
    "The numerical features are generated using random distributions to simulate realistic data:\n",
    "- **Uniform Distribution:** Used for features like `Debt_to_Income_Ratio` and `Credit_Utilization`.\n",
    "- **Normal Distribution:** Used for features like `Annual_Income` and `Loan_Amount` to mimic typical customer distributions.\n",
    "- **Integer Ranges:** Used for features like `Age` and `Years_as_Customer` to ensure realistic values.\n",
    "\n",
    "### **Categorical Features Generation**\n",
    "The categorical features are generated using `np.random.choice` with predefined probabilities to simulate real-world distributions. For instance:\n",
    "- `Gender` assumes a 48%-48%-4% split for \"Male,\" \"Female,\" and \"Non-Binary.\"\n",
    "- `Marital_Status` reflects typical distributions in a population.\n",
    "\n",
    "### **Derived Features**\n",
    "Derived features, such as `Debt_to_Income_Ratio`, `Credit_Utilization`, and `Monthly_Installment`, are calculated based on other attributes to ensure realism.\n",
    "\n",
    "### **Target Variable**\n",
    "The target variable (`Target`) is binary and reflects a 20%-80% split between \"Default\" and \"No Default\" cases, mimicking common bank scenarios.\n",
    "\n",
    "### **Script Execution**\n",
    "The dataset is saved as a CSV file:\n",
    "```python\n",
    "# Save the dataset to a CSV file\n",
    "data.to_csv(\"bank_classification_dataset.csv\", index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Use Case\n",
    "This dataset is suitable for:\n",
    "- Training and testing classification models (e.g., logistic regression, random forests, neural networks).\n",
    "- Practicing feature engineering, data preprocessing, and exploratory data analysis (EDA).\n",
    "- Experimenting with imbalanced classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of rows\n",
    "n_rows = 1_000_000\n",
    "\n",
    "# Create numerical features\n",
    "def generate_numerical_features(n_rows):\n",
    "    return {\n",
    "        \"Age\": np.random.randint(18, 75, size=n_rows),\n",
    "        \"Annual_Income\": np.random.normal(50000, 15000, size=n_rows).round(2),\n",
    "        \"Loan_Amount\": np.random.normal(20000, 10000, size=n_rows).round(2),\n",
    "        \"Savings_Balance\": np.random.normal(10000, 5000, size=n_rows).round(2),\n",
    "        \"Years_as_Customer\": np.random.randint(1, 30, size=n_rows),\n",
    "        \"Credit_Score\": np.random.randint(300, 850, size=n_rows),\n",
    "        \"Debt_to_Income_Ratio\": np.random.uniform(0, 1, size=n_rows).round(2),\n",
    "        \"Credit_Utilization\": np.random.uniform(0, 1, size=n_rows).round(2),\n",
    "        \"Number_of_Credit_Accounts\": np.random.randint(1, 15, size=n_rows),\n",
    "        \"Loan_Term_in_Years\": np.random.randint(1, 30, size=n_rows),\n",
    "        \"Monthly_Installment\": (np.random.normal(2000, 800, size=n_rows)).round(2),\n",
    "        \"Overdraft_Amount\": np.random.normal(5000, 2000, size=n_rows).round(2),\n",
    "        \"Annual_Expenditure\": np.random.normal(40000, 12000, size=n_rows).round(2),\n",
    "    }\n",
    "\n",
    "# Create categorical features\n",
    "def generate_categorical_features(n_rows):\n",
    "    return {\n",
    "        \"Gender\": np.random.choice([\"Male\", \"Female\"], size=n_rows, p=[0.58, 0.42]),\n",
    "        \"Marital_Status\": np.random.choice([\"Single\", \"Married\"], size=n_rows, p=[0.51, 0.49]),\n",
    "        \"Employment_Status\": np.random.choice([\"Employed\", \"Unemployed\", \"Self-Employed\", \"Student\", \"Retired\"], size=n_rows),\n",
    "        \"Education_Level\": np.random.choice([\"High School\", \"Bachelor's\", \"Master's\", \"Doctorate\"], size=n_rows, p=[0.4, 0.4, 0.15, 0.05]),\n",
    "        \"Loan_Purpose\": np.random.choice([\"Car\", \"Home\", \"Education\", \"Personal\", \"Business\"], size=n_rows),\n",
    "        \"Has_Credit_Card\": np.random.choice([\"Yes\", \"No\"], size=n_rows, p=[0.7, 0.3]),\n",
    "        \"Is_Homeowner\": np.random.choice([\"Yes\", \"No\"], size=n_rows, p=[0.6, 0.4]),\n",
    "        \"Account_Type\": np.random.choice([\"Savings\", \"Business\"], size=n_rows, p=[0.65, 0.35]),\n",
    "        \"Customer_Segment\": np.random.choice([\"Premium\", \"Regular\"], size=n_rows, p=[0.52, 0.48]),\n",
    "        \"Preferred_Contact_Channel\": np.random.choice([\"Email\", \"Phone\", \"In-Person\"], size=n_rows, p=[0.5, 0.3, 0.2]),\n",
    "    }\n",
    "\n",
    "# Generate the target variable\n",
    "def generate_target_variable(n_rows):\n",
    "    return np.random.choice([\"Default\", \"No Default\"], size=n_rows, p=[0.2, 0.8])\n",
    "\n",
    "# Combine all features into a DataFrame\n",
    "numerical_features = generate_numerical_features(n_rows)\n",
    "categorical_features = generate_categorical_features(n_rows)\n",
    "\n",
    "data = pd.DataFrame({**numerical_features, **categorical_features})\n",
    "\n",
    "# Add the target variable\n",
    "data[\"Target\"] = generate_target_variable(n_rows)\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "data.to_csv(\"bank_classification_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Dataset generated and saved as 'bank_classification_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = pd.read_csv(\"bank_classification_dataset.csv\")\n",
    "# copy the dataset\n",
    "df = data.copy()\n",
    "#shape of the dataset\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the dataset\n",
    "data_summary = df.describe(include='all')\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display results\n",
    "from IPython.display import display\n",
    "\n",
    "# Display results\n",
    "display(data_summary)\n",
    "display(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Correlation Heatmap for Numerical Features**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "plt.figure(figsize=(12, 8))\n",
    "numerical_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = data[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Heatmap for Numerical Features\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This heatmap visualizes the correlation between all numerical features in the dataset. Correlation values range from -1 to 1:\n",
    "- **1**: Perfect positive correlation.\n",
    "- **-1**: Perfect negative correlation.\n",
    "- **0**: No correlation.\n",
    "\n",
    "**Insights:**\n",
    "- Features with strong positive or negative correlations can indicate multicollinearity, which may need to be addressed during model building.\n",
    "- For example, `Debt-to-Income Ratio` and `Loan Amount` might show a correlation, indicating a potential relationship between a customer's debt and the size of their loan.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style for the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Correlation Heatmap for Numerical Features\n",
    "plt.figure(figsize=(12, 8))\n",
    "numerical_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = data[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Heatmap for Numerical Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Boxplot: Loan Amount vs Loan Purpose**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data, x='Loan_Purpose', y='Loan_Amount')\n",
    "plt.title(\"Loan Amount Distribution by Loan Purpose\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This boxplot shows the distribution of loan amounts for each loan purpose. Each box represents the interquartile range (IQR) of the loan amounts, and the whiskers represent the range, excluding outliers.\n",
    "\n",
    "**Insights:**\n",
    "- You can identify which loan purposes have higher median loan amounts (e.g., `Home` loans might have higher amounts compared to `Car` loans).\n",
    "- Outliers can indicate exceptionally large loans for specific purposes, which might warrant further investigation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Boxplot: Loan Amount vs Loan Purpose\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data, x='Loan_Purpose', y='Loan_Amount')\n",
    "plt.title(\"Loan Amount Distribution by Loan Purpose\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Bar Plot: Target Distribution**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=data, x='Target')\n",
    "plt.title(\"Target Distribution (Default vs No Default)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This bar plot shows the distribution of the target variable (`Default` vs `No Default`). It helps us understand the class balance in the dataset.\n",
    "\n",
    "**Insights:**\n",
    "- If one class is significantly larger than the other (e.g., more `No Default` cases), the dataset is imbalanced. This is common in real-world banking scenarios and requires techniques like oversampling, undersampling, or class weighting during model training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Bar Plot: Target Distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=data, x='Target')\n",
    "plt.title(\"Target Distribution (Default vs No Default)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Scatter Plot: Annual Income vs Loan Amount**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=data, x='Annual_Income', y='Loan_Amount', hue='Target', alpha=0.3)\n",
    "plt.title(\"Annual Income vs Loan Amount (by Target)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This scatter plot visualizes the relationship between `Annual Income` and `Loan Amount`, colored by the target variable (`Default` vs `No Default`).\n",
    "\n",
    "**Insights:**\n",
    "- You can observe clusters of customers with specific income and loan amount combinations.\n",
    "- Defaults may cluster in specific ranges of income and loan amount, such as lower incomes and higher loans, indicating potential risk areas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Scatter Plot: Annual Income vs Loan Amount\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=data, x='Annual_Income', y='Loan_Amount', hue='Target', alpha=0.3)\n",
    "plt.title(\"Annual Income vs Loan Amount (by Target)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Distribution of Debt-to-Income Ratio by Target**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=data, x='Debt_to_Income_Ratio', hue='Target', kde=True, bins=30, alpha=0.5)\n",
    "plt.title(\"Debt-to-Income Ratio Distribution by Target\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This histogram shows the distribution of `Debt-to-Income Ratio` for each target class (`Default` vs `No Default`), with a kernel density estimate (KDE) overlay.\n",
    "\n",
    "**Insights:**\n",
    "- Customers with higher `Debt-to-Income Ratios` are more likely to default, as observed by the peaks in the `Default` category.\n",
    "- The KDE helps visualize the overall distribution and the overlap between the two classes, which is useful for assessing feature separability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Distribution of Debt-to-Income Ratio by Target\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=data, x='Debt_to_Income_Ratio', hue='Target', kde=True, bins=30, alpha=0.5)\n",
    "plt.title(\"Debt-to-Income Ratio Distribution by Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Steps with Explanations and Comparisons\n",
    "\n",
    "## Step 1: Handling Missing Values\n",
    "\n",
    "### Explanation:\n",
    "Missing values can create biases and reduce the predictive power of a machine learning model. For this dataset, we use the following strategies:\n",
    "- **Numerical Features:** Missing values are imputed using the **mean** because it is simple, effective, and preserves the average distribution of the feature.\n",
    "  - **Alternative:** Median imputation is robust to outliers but may not reflect the central tendency well if the data is normally distributed.\n",
    "- **Categorical Features:** Missing values are imputed using the **most frequent value** because it is computationally efficient and preserves the mode of the data.\n",
    "  - **Alternative:** K-Nearest Neighbors (KNN) imputation considers neighboring data points but is computationally expensive and may introduce noise.\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "# Impute missing values for numerical and categorical features\n",
    "numerical_features = data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "categorical_features = data.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy=\"mean\")  # Mean for numerical features\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")  # Mode for categorical features\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Ensure column names are clean\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Step 5: Separate Features and Target Variable\n",
    "if \"Target\" not in df.columns:\n",
    "    raise ValueError(\"The column 'Target' is missing from the dataset.\")\n",
    "\n",
    "X = df.drop(columns=[\"Target\"])  # Exclude the target variable\n",
    "y = df[\"Target\"]  # Target variable remains unchanged\n",
    "\n",
    "# Dynamically define numerical and categorical features based on X\n",
    "numerical_features = X.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy=\"mean\")  # Mean for numerical features\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")  # Mode for categorical features\n",
    "# what is impuation \n",
    "# Imputation is the process of replacing missing data with substituted values. When substituting for a missing value,\n",
    "# we can use the mean, the median, or the mode of the non-missing values.\n",
    "# Imputation preserves the sample size of the data, which results in more accurate statistical analysis.\n",
    "# Imputation is a better option than dropping missing values from the dataset, as it preserves the sample size and\n",
    "# ensures that the data is not lost.\n",
    "\n",
    "print(\"Categorical features :\",categorical_features)\n",
    "print('-'*250)\n",
    "print(\"numerical features :\",numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking unique values in categorical features\n",
    "X[categorical_features].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking unique values in numerical features\n",
    "X[numerical_features].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "# Ordinal Encoding\n",
    "ordinal_cols = ['Education_Level', 'Loan_Purpose']\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X[ordinal_cols] = ordinal_encoder.fit_transform(X[ordinal_cols])\n",
    "\n",
    "# Label Encoding\n",
    "label_cols = ['Gender', 'Marital_Status', 'Employment_Status', 'Has_Credit_Card', \n",
    "              'Is_Homeowner', 'Account_Type', 'Customer_Segment', 'Preferred_Contact_Channel']\n",
    "label_encoder = LabelEncoder()\n",
    "for col in label_cols:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "# Also encoding the target variable\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Display the first few rows of the transformed dataset\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispay target variable\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking outliners in numerical features\n",
    "plt.figure(figsize=(18, 12))\n",
    "X[numerical_features].boxplot()\n",
    "plt.title(\"Boxplot of Numerical Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to cap outliers\n",
    "def cap_outliers(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Cap outliers\n",
    "    df_capped = df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "    return df_capped\n",
    "\n",
    "# Cap outliers in numerical features\n",
    "X[numerical_features] = cap_outliers(X[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the boxplot after capping the outliners\n",
    "plt.figure(figsize=(18, 12))\n",
    "X[numerical_features].boxplot()\n",
    "plt.title(\"Boxplot of Numerical Features (After Capping Outliers)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for class imbalance\n",
    "class_distribution = df[\"Target\"].value_counts(normalize=True) * 100\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling class imbalance before splitting the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Display the class distribution after applying SMOTE\n",
    "y_resampled_distribution = pd.Series(y_resampled).value_counts(normalize=True) * 100\n",
    "print(y_resampled_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the training and testing sets \n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('+'*50)\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print('+'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importance = rf_classifier.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\"Feature\": X.columns, \"Importance\": feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot the Feature Importance   \n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importance_df, x=\"Importance\", y=\"Feature\", palette=\"viridis\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting sensitive features as categorical features\n",
    "# remove loan purpose and education level from sensitive features\n",
    "sensitive_features = [feature for feature in categorical_features if feature not in ['Loan_Purpose', 'Education_Level']]\n",
    "print(sensitive_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import shutil\n",
    "import csv\n",
    "from tempfile import gettempdir\n",
    "\n",
    "from tqdm import tqdm\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from bokeh.io import output_file, save\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import ColumnDataSource\n",
    "from fairlearn.metrics import selection_rate, count, false_positive_rate, false_negative_rate, true_positive_rate, true_negative_rate\n",
    "# Create the plots directory\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Prepare the report data\n",
    "report_data = []\n",
    "\n",
    "\n",
    "def generate_screenshot(html_file, output_png):\n",
    "    \"\"\"\n",
    "    Detects the best browser, constructs the command, and generates a screenshot of the given HTML file.\n",
    "\n",
    "    Parameters:\n",
    "        html_file (str): Path to the input HTML file.\n",
    "        output_png (str): Path to save the output PNG screenshot.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the screenshot was successfully generated, False otherwise.\n",
    "    \"\"\"\n",
    "    browser_priority = [\"chrome\", \"edge\", \"safari\", \"firefox\", \"opera\"]\n",
    "    browser_paths = {\n",
    "        \"chrome\": [\n",
    "            \"chrome\", \"google-chrome\", \"chrome.exe\",\n",
    "            r\"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\",\n",
    "            r\"C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"\n",
    "        ],\n",
    "        \"edge\": [\n",
    "            \"msedge\", \"msedge.exe\",\n",
    "            r\"C:\\\\Program Files (x86)\\\\Microsoft\\\\Edge\\\\Application\\\\msedge.exe\",\n",
    "            r\"C:\\\\Program Files\\\\Microsoft\\\\Edge\\\\Application\\\\msedge.exe\"\n",
    "        ],\n",
    "        \"safari\": [\"/Applications/Safari.app/Contents/MacOS/Safari\"],\n",
    "        \"firefox\": [\n",
    "            \"firefox\", \"firefox.exe\",\n",
    "            r\"C:\\\\Program Files\\\\Mozilla Firefox\\\\firefox.exe\",\n",
    "            r\"C:\\\\Program Files (x86)\\\\Mozilla Firefox\\\\firefox.exe\"\n",
    "        ],\n",
    "        \"opera\": [\n",
    "            \"opera\", \"opera.exe\",\n",
    "            r\"C:\\\\Program Files\\\\Opera\\\\launcher.exe\",\n",
    "            r\"C:\\\\Program Files (x86)\\\\Opera\\\\launcher.exe\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def find_browser():\n",
    "        for browser in browser_priority:\n",
    "            for executable in browser_paths.get(browser, []):\n",
    "                path = shutil.which(executable) or (\n",
    "                    executable if os.path.isfile(executable) else None)\n",
    "                if path:\n",
    "                    return browser, path\n",
    "        return None, None\n",
    "\n",
    "    def build_command(browser, browser_path, html_file, output_png):\n",
    "        preset_width, preset_height = 1280,520\n",
    "        if browser in [\"chrome\", \"edge\", \"opera\"]:\n",
    "            return [\n",
    "                browser_path, \"--headless\", \"--disable-gpu\",\n",
    "                f\"--window-size={preset_width},{preset_height}\",\n",
    "                f\"--screenshot={output_png}\",\n",
    "                f\"file:///{html_file}\"\n",
    "            ]\n",
    "        elif browser == \"firefox\":\n",
    "            return [\n",
    "                browser_path, \"--headless\",\n",
    "                \"--window-size\", f\"{preset_width},{preset_height}\",\n",
    "                \"--screenshot\", output_png,\n",
    "                f\"file:///{html_file}\"\n",
    "            ]\n",
    "        elif browser == \"safari\":\n",
    "            raise NotImplementedError(\n",
    "                \"Safari does not natively support headless mode.\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported browser for headless mode: {browser}\")\n",
    "\n",
    "    def execute_command(command):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                command, check=True, capture_output=True, text=True)\n",
    "            return True\n",
    "        except subprocess.CalledProcessError:\n",
    "            return False\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    if not os.path.isfile(html_file):\n",
    "        return False\n",
    "\n",
    "    browser, browser_path = find_browser()\n",
    "    if not browser or not browser_path:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        command = build_command(browser, browser_path, html_file, output_png)\n",
    "        return execute_command(command)\n",
    "    except (NotImplementedError, ValueError):\n",
    "        return False\n",
    "\n",
    "\n",
    "# Start the main processing\n",
    "y_true = y_test\n",
    "start_time = time.time()\n",
    "\n",
    "total_rows = len(y_test)\n",
    "sensitive_columns = sensitive_features\n",
    "\n",
    "# Main processing loop\n",
    "for i in tqdm(sensitive_columns, desc=\"Processing sensitive columns\", total=len(sensitive_columns)):\n",
    "    iteration_start_time = time.time()\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score,\n",
    "        \"Precision\": lambda y_true, y_pred: precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"False Positive Rate\": false_positive_rate,\n",
    "        \"False Negative Rate\": false_negative_rate,\n",
    "        \"True Positive Rate\": true_positive_rate,\n",
    "        \"True Negative Rate\": true_negative_rate,\n",
    "        \"Selection Rate\": selection_rate,\n",
    "        \"Count\": count,\n",
    "    }\n",
    "\n",
    "    protected_class = X_test[i]\n",
    "    metric_frame = MetricFrame(\n",
    "        metrics=metrics,\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred,\n",
    "        sensitive_features=protected_class\n",
    "    )\n",
    "\n",
    "    iteration_end_time = time.time()\n",
    "    metric_time = iteration_end_time - iteration_start_time\n",
    "    print(\n",
    "        f\"Time taken for metrics calculation (feature '{i}'): {metric_time:.4f} seconds\")\n",
    "\n",
    "    metrics_data = metric_frame.by_group.reset_index()\n",
    "    sensitive_feature_column = metrics_data.columns[0]\n",
    "    metrics_data = metrics_data.melt(\n",
    "        id_vars=[sensitive_feature_column], var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "    colors = [\n",
    "        \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\",\n",
    "        \"#9467bd\", \"#8c564b\", \"#e377c2\", \"#7f7f7f\"\n",
    "    ]\n",
    "\n",
    "    plots = []\n",
    "    for idx, metric in enumerate(metrics_data['Metric'].unique()):\n",
    "        data = metrics_data[metrics_data['Metric'] == metric]\n",
    "        data[sensitive_feature_column] = data[sensitive_feature_column].astype(\n",
    "            str)\n",
    "        source = ColumnDataSource(data)\n",
    "\n",
    "        p = figure(\n",
    "            title=metric,\n",
    "            x_range=data[sensitive_feature_column].unique(),\n",
    "            height=200,\n",
    "            width=300\n",
    "        )\n",
    "        p.vbar(x=sensitive_feature_column, top='Value', width=0.9,\n",
    "               source=source, color=colors[idx % len(colors)])\n",
    "        p.xaxis.axis_label = sensitive_feature_column\n",
    "        p.yaxis.axis_label = metric\n",
    "        p.title.text_font_size = \"14pt\"\n",
    "        plots.append(p)\n",
    "\n",
    "    grid = gridplot([plots[:4], plots[4:]])\n",
    "\n",
    "    html_file = os.path.abspath(os.path.join(\n",
    "        \"plots\", f\"metrics_bokeh_{i}.html\"))\n",
    "    output_file(html_file)\n",
    "    save(grid)\n",
    "\n",
    "    png_start_time = time.time()\n",
    "    output_png = html_file.replace(\".html\", \".png\")\n",
    "    success = generate_screenshot(html_file, output_png)\n",
    "    png_end_time = time.time()\n",
    "\n",
    "    image_time = png_end_time - png_start_time\n",
    "    if success:\n",
    "        print(\n",
    "            f\"Time taken to save PNG (feature '{i}'): {image_time:.4f} seconds\")\n",
    "    else:\n",
    "        print(f\"Failed to generate PNG for feature '{i}'\")\n",
    "\n",
    "    # Add data to the report\n",
    "    report_data.append({\n",
    "        \"Sensitive Column\": i,\n",
    "        \"Metric Calculation Time (s)\": metric_time,\n",
    "        \"Image Generation Time (s)\": image_time,\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total execution time: {total_time:.4f} seconds\")\n",
    "\n",
    "# Add summary to the report\n",
    "total_sensitive_columns = len(sensitive_columns)\n",
    "report_data.append({\n",
    "    \"Sensitive Column\": \"Summary\",\n",
    "    \"Metric Calculation Time (s)\": \"---\",\n",
    "    \"Image Generation Time (s)\": \"---\",\n",
    "    \"Total Sensitive Columns\": total_sensitive_columns,\n",
    "    \"Total Rows\": total_rows,\n",
    "    \"Total Execution Time (s)\": total_time\n",
    "})\n",
    "\n",
    "# Write the report to a CSV file\n",
    "report_file = os.path.join(\"plots\", \"report.csv\")\n",
    "with open(report_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        \"Sensitive Column\", \"Metric Calculation Time (s)\", \"Image Generation Time (s)\",\n",
    "        \"Total Sensitive Columns\", \"Total Rows\", \"Total Execution Time (s)\"\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(report_data)\n",
    "\n",
    "print(f\"Report saved to {report_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cimcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
