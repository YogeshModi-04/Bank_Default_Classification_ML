{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the Bank Classification Dataset\n",
    "\n",
    "## Overview\n",
    "This document provides a detailed explanation of the synthetic dataset generated for a bank-related classification problem. The dataset is designed to simulate real-world scenarios where a bank predicts customer behavior, such as loan default or creditworthiness. It includes a mix of numerical and categorical features, ensuring that all columns are meaningful and relevant to the banking domain.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Details\n",
    "### **Rows and Columns**\n",
    "- **Number of Rows:** 1,000,000\n",
    "- **Number of Columns:** 25 (excluding the target variable)\n",
    "\n",
    "### **Target Variable**\n",
    "- **Name:** `Target`\n",
    "- **Type:** Categorical\n",
    "- **Values:**\n",
    "  - `Default`: Represents customers who default on their loans.\n",
    "  - `No Default`: Represents customers who do not default.\n",
    "\n",
    "### **Numerical Features**\n",
    "1. **Age**: Customer's age in years (range: 18-75).\n",
    "2. **Annual_Income**: Customer's annual income in USD, following a normal distribution centered around $50,000 with a standard deviation of $15,000.\n",
    "3. **Loan_Amount**: The amount of the loan issued in USD, normally distributed around $20,000 with a standard deviation of $10,000.\n",
    "4. **Savings_Balance**: Customer's current savings account balance in USD, normally distributed around $10,000 with a standard deviation of $5,000.\n",
    "5. **Years_as_Customer**: Number of years the customer has been with the bank (range: 1-30).\n",
    "6. **Credit_Score**: Customer's credit score (range: 300-850).\n",
    "7. **Debt_to_Income_Ratio**: Debt-to-income ratio, a derived feature (range: 0.0-1.0).\n",
    "8. **Credit_Utilization**: Ratio of credit used to the total available credit (range: 0.0-1.0).\n",
    "9. **Number_of_Credit_Accounts**: Number of credit accounts held by the customer (range: 1-15).\n",
    "10. **Loan_Term_in_Years**: Duration of the loan in years (range: 1-30).\n",
    "11. **Monthly_Installment**: Monthly installment payment in USD, derived based on loan amount and term.\n",
    "12. **Overdraft_Amount**: Overdraft amount used by the customer in USD (mean: $5,000, standard deviation: $2,000).\n",
    "13. **Annual_Expenditure**: Customer's annual expenditure in USD (mean: $40,000, standard deviation: $12,000).\n",
    "\n",
    "### **Categorical Features**\n",
    "1. **Gender**: Customer's gender (“Male”, “Female”, “Non-Binary”).\n",
    "2. **Marital_Status**: Customer's marital status (“Single”, “Married”, “Divorced”, “Widowed”).\n",
    "3. **Employment_Status**: Employment status (“Employed”, “Unemployed”, “Self-Employed”, “Student”, “Retired”).\n",
    "4. **Education_Level**: Highest level of education completed (“High School”, “Bachelor’s”, “Master’s”, “Doctorate”).\n",
    "5. **Loan_Purpose**: Purpose of the loan (“Car”, “Home”, “Education”, “Personal”, “Business”).\n",
    "6. **Has_Credit_Card**: Indicates whether the customer owns a credit card (“Yes”, “No”).\n",
    "7. **Is_Homeowner**: Indicates whether the customer owns a home (“Yes”, “No”).\n",
    "8. **Account_Type**: Type of account held by the customer (“Savings”, “Checking”, “Business”).\n",
    "9. **Customer_Segment**: Categorizes customers into segments (“Premium”, “Regular”, “Occasional”).\n",
    "10. **Preferred_Contact_Channel**: Preferred method of contact (“Email”, “Phone”, “In-Person”).\n",
    "\n",
    "---\n",
    "\n",
    "## Code Explanation\n",
    "### **Numerical Features Generation**\n",
    "The numerical features are generated using random distributions to simulate realistic data:\n",
    "- **Uniform Distribution:** Used for features like `Debt_to_Income_Ratio` and `Credit_Utilization`.\n",
    "- **Normal Distribution:** Used for features like `Annual_Income` and `Loan_Amount` to mimic typical customer distributions.\n",
    "- **Integer Ranges:** Used for features like `Age` and `Years_as_Customer` to ensure realistic values.\n",
    "\n",
    "### **Categorical Features Generation**\n",
    "The categorical features are generated using `np.random.choice` with predefined probabilities to simulate real-world distributions. For instance:\n",
    "- `Gender` assumes a 48%-48%-4% split for \"Male,\" \"Female,\" and \"Non-Binary.\"\n",
    "- `Marital_Status` reflects typical distributions in a population.\n",
    "\n",
    "### **Derived Features**\n",
    "Derived features, such as `Debt_to_Income_Ratio`, `Credit_Utilization`, and `Monthly_Installment`, are calculated based on other attributes to ensure realism.\n",
    "\n",
    "### **Target Variable**\n",
    "The target variable (`Target`) is binary and reflects a 20%-80% split between \"Default\" and \"No Default\" cases, mimicking common bank scenarios.\n",
    "\n",
    "### **Script Execution**\n",
    "The dataset is saved as a CSV file:\n",
    "```python\n",
    "# Save the dataset to a CSV file\n",
    "data.to_csv(\"bank_classification_dataset.csv\", index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Use Case\n",
    "This dataset is suitable for:\n",
    "- Training and testing classification models (e.g., logistic regression, random forests, neural networks).\n",
    "- Practicing feature engineering, data preprocessing, and exploratory data analysis (EDA).\n",
    "- Experimenting with imbalanced classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of rows\n",
    "n_rows = 1_000_000\n",
    "\n",
    "# Create numerical features\n",
    "def generate_numerical_features(n_rows):\n",
    "    return {\n",
    "        \"Age\": np.random.randint(18, 75, size=n_rows),\n",
    "        \"Annual_Income\": np.random.normal(50000, 15000, size=n_rows).round(2),\n",
    "        \"Loan_Amount\": np.random.normal(20000, 10000, size=n_rows).round(2),\n",
    "        \"Savings_Balance\": np.random.normal(10000, 5000, size=n_rows).round(2),\n",
    "        \"Years_as_Customer\": np.random.randint(1, 30, size=n_rows),\n",
    "        \"Credit_Score\": np.random.randint(300, 850, size=n_rows),\n",
    "        \"Debt_to_Income_Ratio\": np.random.uniform(0, 1, size=n_rows).round(2),\n",
    "        \"Credit_Utilization\": np.random.uniform(0, 1, size=n_rows).round(2),\n",
    "        \"Number_of_Credit_Accounts\": np.random.randint(1, 15, size=n_rows),\n",
    "        \"Loan_Term_in_Years\": np.random.randint(1, 30, size=n_rows),\n",
    "        \"Monthly_Installment\": (np.random.normal(2000, 800, size=n_rows)).round(2),\n",
    "        \"Overdraft_Amount\": np.random.normal(5000, 2000, size=n_rows).round(2),\n",
    "        \"Annual_Expenditure\": np.random.normal(40000, 12000, size=n_rows).round(2),\n",
    "    }\n",
    "\n",
    "# Create categorical features\n",
    "def generate_categorical_features(n_rows):\n",
    "    return {\n",
    "        \"Gender\": np.random.choice([\"Male\", \"Female\"], size=n_rows, p=[0.58, 0.42]),\n",
    "        \"Marital_Status\": np.random.choice([\"Single\", \"Married\"], size=n_rows, p=[0.51, 0.49]),\n",
    "        \"Employment_Status\": np.random.choice([\"Employed\", \"Unemployed\", \"Self-Employed\", \"Student\", \"Retired\"], size=n_rows),\n",
    "        \"Education_Level\": np.random.choice([\"High School\", \"Bachelor's\", \"Master's\", \"Doctorate\"], size=n_rows, p=[0.4, 0.4, 0.15, 0.05]),\n",
    "        \"Loan_Purpose\": np.random.choice([\"Car\", \"Home\", \"Education\", \"Personal\", \"Business\"], size=n_rows),\n",
    "        \"Has_Credit_Card\": np.random.choice([\"Yes\", \"No\"], size=n_rows, p=[0.7, 0.3]),\n",
    "        \"Is_Homeowner\": np.random.choice([\"Yes\", \"No\"], size=n_rows, p=[0.6, 0.4]),\n",
    "        \"Account_Type\": np.random.choice([\"Savings\", \"Business\"], size=n_rows, p=[0.65, 0.35]),\n",
    "        \"Customer_Segment\": np.random.choice([\"Premium\", \"Regular\"], size=n_rows, p=[0.52, 0.48]),\n",
    "        \"Preferred_Contact_Channel\": np.random.choice([\"Email\", \"Phone\", \"In-Person\"], size=n_rows, p=[0.5, 0.3, 0.2]),\n",
    "    }\n",
    "\n",
    "# Generate the target variable\n",
    "def generate_target_variable(n_rows):\n",
    "    return np.random.choice([\"Default\", \"No Default\"], size=n_rows, p=[0.2, 0.8])\n",
    "\n",
    "# Combine all features into a DataFrame\n",
    "numerical_features = generate_numerical_features(n_rows)\n",
    "categorical_features = generate_categorical_features(n_rows)\n",
    "\n",
    "data = pd.DataFrame({**numerical_features, **categorical_features})\n",
    "\n",
    "# Add the target variable\n",
    "data[\"Target\"] = generate_target_variable(n_rows)\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "data.to_csv(\"bank_classification_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Dataset generated and saved as 'bank_classification_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = pd.read_csv(\"bank_classification_dataset.csv\")\n",
    "# copy the dataset\n",
    "df = data.copy()\n",
    "#shape of the dataset\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the dataset\n",
    "data_summary = df.describe(include='all')\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display results\n",
    "from IPython.display import display\n",
    "\n",
    "# Display results\n",
    "display(data_summary)\n",
    "display(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Correlation Heatmap for Numerical Features**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "plt.figure(figsize=(12, 8))\n",
    "numerical_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = data[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Heatmap for Numerical Features\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This heatmap visualizes the correlation between all numerical features in the dataset. Correlation values range from -1 to 1:\n",
    "- **1**: Perfect positive correlation.\n",
    "- **-1**: Perfect negative correlation.\n",
    "- **0**: No correlation.\n",
    "\n",
    "**Insights:**\n",
    "- Features with strong positive or negative correlations can indicate multicollinearity, which may need to be addressed during model building.\n",
    "- For example, `Debt-to-Income Ratio` and `Loan Amount` might show a correlation, indicating a potential relationship between a customer's debt and the size of their loan.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style for the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Correlation Heatmap for Numerical Features\n",
    "plt.figure(figsize=(12, 8))\n",
    "numerical_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = data[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Heatmap for Numerical Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Boxplot: Loan Amount vs Loan Purpose**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data, x='Loan_Purpose', y='Loan_Amount')\n",
    "plt.title(\"Loan Amount Distribution by Loan Purpose\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This boxplot shows the distribution of loan amounts for each loan purpose. Each box represents the interquartile range (IQR) of the loan amounts, and the whiskers represent the range, excluding outliers.\n",
    "\n",
    "**Insights:**\n",
    "- You can identify which loan purposes have higher median loan amounts (e.g., `Home` loans might have higher amounts compared to `Car` loans).\n",
    "- Outliers can indicate exceptionally large loans for specific purposes, which might warrant further investigation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Boxplot: Loan Amount vs Loan Purpose\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data, x='Loan_Purpose', y='Loan_Amount')\n",
    "plt.title(\"Loan Amount Distribution by Loan Purpose\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Bar Plot: Target Distribution**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=data, x='Target')\n",
    "plt.title(\"Target Distribution (Default vs No Default)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This bar plot shows the distribution of the target variable (`Default` vs `No Default`). It helps us understand the class balance in the dataset.\n",
    "\n",
    "**Insights:**\n",
    "- If one class is significantly larger than the other (e.g., more `No Default` cases), the dataset is imbalanced. This is common in real-world banking scenarios and requires techniques like oversampling, undersampling, or class weighting during model training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Bar Plot: Target Distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=data, x='Target')\n",
    "plt.title(\"Target Distribution (Default vs No Default)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Scatter Plot: Annual Income vs Loan Amount**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=data, x='Annual_Income', y='Loan_Amount', hue='Target', alpha=0.3)\n",
    "plt.title(\"Annual Income vs Loan Amount (by Target)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This scatter plot visualizes the relationship between `Annual Income` and `Loan Amount`, colored by the target variable (`Default` vs `No Default`).\n",
    "\n",
    "**Insights:**\n",
    "- You can observe clusters of customers with specific income and loan amount combinations.\n",
    "- Defaults may cluster in specific ranges of income and loan amount, such as lower incomes and higher loans, indicating potential risk areas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Scatter Plot: Annual Income vs Loan Amount\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=data, x='Annual_Income', y='Loan_Amount', hue='Target', alpha=0.3)\n",
    "plt.title(\"Annual Income vs Loan Amount (by Target)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Distribution of Debt-to-Income Ratio by Target**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=data, x='Debt_to_Income_Ratio', hue='Target', kde=True, bins=30, alpha=0.5)\n",
    "plt.title(\"Debt-to-Income Ratio Distribution by Target\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This histogram shows the distribution of `Debt-to-Income Ratio` for each target class (`Default` vs `No Default`), with a kernel density estimate (KDE) overlay.\n",
    "\n",
    "**Insights:**\n",
    "- Customers with higher `Debt-to-Income Ratios` are more likely to default, as observed by the peaks in the `Default` category.\n",
    "- The KDE helps visualize the overall distribution and the overlap between the two classes, which is useful for assessing feature separability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Distribution of Debt-to-Income Ratio by Target\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=data, x='Debt_to_Income_Ratio', hue='Target', kde=True, bins=30, alpha=0.5)\n",
    "plt.title(\"Debt-to-Income Ratio Distribution by Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Steps with Explanations and Comparisons\n",
    "\n",
    "## Step 1: Handling Missing Values\n",
    "\n",
    "### Explanation:\n",
    "Missing values can create biases and reduce the predictive power of a machine learning model. For this dataset, we use the following strategies:\n",
    "- **Numerical Features:** Missing values are imputed using the **mean** because it is simple, effective, and preserves the average distribution of the feature.\n",
    "  - **Alternative:** Median imputation is robust to outliers but may not reflect the central tendency well if the data is normally distributed.\n",
    "- **Categorical Features:** Missing values are imputed using the **most frequent value** because it is computationally efficient and preserves the mode of the data.\n",
    "  - **Alternative:** K-Nearest Neighbors (KNN) imputation considers neighboring data points but is computationally expensive and may introduce noise.\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "# Impute missing values for numerical and categorical features\n",
    "numerical_features = data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "categorical_features = data.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy=\"mean\")  # Mean for numerical features\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")  # Mode for categorical features\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Ensure column names are clean\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Step 5: Separate Features and Target Variable\n",
    "if \"Target\" not in df.columns:\n",
    "    raise ValueError(\"The column 'Target' is missing from the dataset.\")\n",
    "\n",
    "X = df.drop(columns=[\"Target\"])  # Exclude the target variable\n",
    "y = df[\"Target\"]  # Target variable remains unchanged\n",
    "\n",
    "# Dynamically define numerical and categorical features based on X\n",
    "numerical_features = X.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy=\"mean\")  # Mean for numerical features\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")  # Mode for categorical features\n",
    "# what is impuation \n",
    "# Imputation is the process of replacing missing data with substituted values. When substituting for a missing value,\n",
    "# we can use the mean, the median, or the mode of the non-missing values.\n",
    "# Imputation preserves the sample size of the data, which results in more accurate statistical analysis.\n",
    "# Imputation is a better option than dropping missing values from the dataset, as it preserves the sample size and\n",
    "# ensures that the data is not lost.\n",
    "\n",
    "print(\"Categorical features :\",categorical_features)\n",
    "print('-'*250)\n",
    "print(\"numerical features :\",numerical_features)\n",
    "print('-'*250)\n",
    "print(\"Unique Values in Categorical features :\",X[categorical_features].nunique())\n",
    "print('-'*250)\n",
    "print(\"Unique values in Numerical features :\",X[numerical_features].nunique())\n",
    "print('-'*250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Explanation of Encoding in the Code\n",
    "\n",
    "## Code:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "# Ordinal Encoding\n",
    "ordinal_cols = ['Education_Level', 'Loan_Purpose']\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X[ordinal_cols] = ordinal_encoder.fit_transform(X[ordinal_cols])\n",
    "\n",
    "# Label Encoding\n",
    "label_cols = ['Gender', 'Marital_Status', 'Employment_Status', 'Has_Credit_Card', \n",
    "              'Is_Homeowner', 'Account_Type', 'Customer_Segment', 'Preferred_Contact_Channel']\n",
    "label_encoder = LabelEncoder()\n",
    "for col in label_cols:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "# Also encoding the target variable\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Display the first few rows of the transformed dataset\n",
    "X.head()\n",
    "```\n",
    "\n",
    "## What This Code Does:\n",
    "\n",
    "This code transforms \"categorical data\" (data that consists of categories like \"Male/Female\" or \"Yes/No\") into a numeric format so that machine learning algorithms can understand it.\n",
    "\n",
    "### Why Encoding is Necessary\n",
    "Machine learning models can work only with numbers, not text. For example, a model won't understand words like \"Male\" or \"Female,\" but it can work with numbers like `0` and `1`.\n",
    "\n",
    "### What Types of Encoding Are Used Here?\n",
    "\n",
    "1. **Ordinal Encoding**:\n",
    "   - Used for features (columns) that have a natural order. For example, \"Education Level\" might include categories like:\n",
    "     - \"High School\"\n",
    "     - \"Undergraduate\"\n",
    "     - \"Postgraduate\"\n",
    "   - These are converted into numbers such as:\n",
    "     - High School → 0\n",
    "     - Undergraduate → 1\n",
    "     - Postgraduate → 2\n",
    "\n",
    "2. **Label Encoding**:\n",
    "   - Used for features with no specific order, like \"Gender\" or \"Marital Status.\" For example:\n",
    "     - Gender:\n",
    "       - Male → 0\n",
    "       - Female → 1\n",
    "     - Marital Status:\n",
    "       - Single → 0\n",
    "       - Married → 1\n",
    "\n",
    "   - Similarly, the target variable (`y`) is also encoded to turn the labels into numbers.\n",
    "\n",
    "### Why Not Use One-Hot Encoding?\n",
    "One-hot encoding is another way to transform categories into numbers, but it creates additional columns for each category. For example:\n",
    "   - \"Male\" and \"Female\" would become two separate columns like:\n",
    "     - Male: [1, 0]\n",
    "     - Female: [0, 1]\n",
    "   \n",
    "   Since this dataset's categorical features mostly have binary values or only a few unique categories, one-hot encoding would create unnecessary additional columns, increasing the dataset's size without adding much value.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Import Libraries**:\n",
    "   - The code imports tools (`OrdinalEncoder` and `LabelEncoder`) from `sklearn`, a popular library for machine learning in Python.\n",
    "\n",
    "2. **Ordinal Encoding**:\n",
    "   - Two columns, `'Education_Level'` and `'Loan_Purpose'`, are identified as having a natural order.\n",
    "   - They are encoded using `OrdinalEncoder`.\n",
    "\n",
    "3. **Label Encoding**:\n",
    "   - A list of other categorical columns (like `'Gender'`, `'Marital_Status'`, etc.) is encoded using `LabelEncoder`.\n",
    "   - Each category is replaced with a number.\n",
    "   - This is done using a loop to apply the encoding to each column.\n",
    "\n",
    "4. **Target Variable Encoding**:\n",
    "   - The target variable (`y`), which the model predicts, is also encoded into numeric format using `LabelEncoder`.\n",
    "\n",
    "5. **View Transformed Data**:\n",
    "   - Finally, the first few rows of the transformed dataset are displayed with `X.head()`.\n",
    "\n",
    "### Why This Approach Was Chosen:\n",
    "- **Efficiency**: Encoding with numbers keeps the dataset simple and compact.\n",
    "- **Suitability**: Since most of the categorical data has a small number of unique values (like \"Yes/No\"), using `LabelEncoder` and `OrdinalEncoder` avoids the unnecessary complexity of creating extra columns, as one-hot encoding would.\n",
    "- **Compatibility**: The encoded data is now ready to be used with machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "# Ordinal Encoding\n",
    "ordinal_cols = ['Education_Level', 'Loan_Purpose']\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X[ordinal_cols] = ordinal_encoder.fit_transform(X[ordinal_cols])\n",
    "\n",
    "# Label Encoding\n",
    "label_cols = ['Gender', 'Marital_Status', 'Employment_Status', 'Has_Credit_Card', \n",
    "              'Is_Homeowner', 'Account_Type', 'Customer_Segment', 'Preferred_Contact_Channel']\n",
    "label_encoder = LabelEncoder()\n",
    "for col in label_cols:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "# Also encoding the target variable\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Display the first few rows of the transformed dataset\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Checking Outliers in Numerical Features\n",
    "\n",
    "## Code:\n",
    "\n",
    "```python\n",
    "# Checking outliers in numerical features\n",
    "plt.figure(figsize=(18, 12))\n",
    "X[numerical_features].boxplot()\n",
    "plt.title(\"Boxplot of Numerical Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## What This Code Does:\n",
    "\n",
    "This code visualizes the numerical features of the dataset to check for outliers using a **boxplot**. Outliers are data points that are significantly different from the rest of the data. Detecting and handling outliers is important because they can negatively impact the performance of machine learning models.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Set the Figure Size**:\n",
    "   - `plt.figure(figsize=(18, 12))` sets the size of the figure to be large enough to display the boxplots for all numerical features clearly.\n",
    "\n",
    "2. **Boxplot Creation**:\n",
    "   - `X[numerical_features].boxplot()` creates a boxplot for each numerical feature in the dataset (`numerical_features` is a list of column names containing numerical data).\n",
    "   - A boxplot is a statistical graph that shows the distribution of data:\n",
    "     - The box represents the interquartile range (IQR), where 50% of the data lies.\n",
    "     - The line inside the box shows the median.\n",
    "     - Points outside the whiskers are potential outliers.\n",
    "\n",
    "3. **Title**:\n",
    "   - `plt.title(\"Boxplot of Numerical Features\")` adds a title to the plot for better readability.\n",
    "\n",
    "4. **Rotate X-Axis Labels**:\n",
    "   - `plt.xticks(rotation=45)` rotates the labels on the x-axis by 45 degrees. This helps when the feature names are long and need to be displayed more clearly.\n",
    "\n",
    "5. **Display the Plot**:\n",
    "   - `plt.show()` renders the boxplot.\n",
    "\n",
    "### Why Use a Boxplot?\n",
    "- **Outlier Detection**: Boxplots are an effective way to visually identify outliers in numerical data. Outliers are usually shown as individual points outside the whiskers.\n",
    "- **Distribution Overview**: Boxplots provide a summary of the distribution of data, including the median, range, and variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking outliners in numerical features\n",
    "plt.figure(figsize=(18, 12))\n",
    "X[numerical_features].boxplot()\n",
    "plt.title(\"Boxplot of Numerical Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Capping Outliers in Numerical Features\n",
    "\n",
    "## Code:\n",
    "\n",
    "```python\n",
    "# Define a function to cap outliers\n",
    "def cap_outliers(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Cap outliers\n",
    "    df_capped = df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "    return df_capped\n",
    "\n",
    "# Cap outliers in numerical features\n",
    "X[numerical_features] = cap_outliers(X[numerical_features])\n",
    "```\n",
    "\n",
    "## What This Code Does:\n",
    "\n",
    "This code identifies and modifies extreme values (outliers) in numerical features of a dataset to bring them within a reasonable range. It uses a method based on the **Interquartile Range (IQR)** to determine what counts as an outlier and then \"caps\" (or replaces) those values with the calculated boundaries.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Define the Function**:\n",
    "   - The function `cap_outliers(df)` takes a DataFrame (`df`) as input and adjusts any extreme values in its columns.\n",
    "\n",
    "2. **Calculate Quartiles**:\n",
    "   - `Q1 = df.quantile(0.25)` calculates the 25th percentile (lower quartile).\n",
    "   - `Q3 = df.quantile(0.75)` calculates the 75th percentile (upper quartile).\n",
    "   - The Interquartile Range (IQR) is calculated as `IQR = Q3 - Q1`. This range contains the middle 50% of the data.\n",
    "\n",
    "3. **Determine Bounds**:\n",
    "   - **Lower Bound**: `Q1 - 1.5 * IQR` — values below this are considered outliers.\n",
    "   - **Upper Bound**: `Q3 + 1.5 * IQR` — values above this are considered outliers.\n",
    "\n",
    "4. **Cap Outliers**:\n",
    "   - `df.clip(lower=lower_bound, upper=upper_bound, axis=1)` replaces values below the lower bound with the lower bound and values above the upper bound with the upper bound.\n",
    "\n",
    "5. **Apply the Function**:\n",
    "   - The function is applied to the numerical features of the dataset (`X[numerical_features]`), and the capped data replaces the original values.\n",
    "\n",
    "### Why Cap Outliers?\n",
    "\n",
    "- **Improves Model Performance**: Extreme values can skew the results of machine learning models, leading to poor predictions.\n",
    "- **Retains Data**: Instead of removing outliers, this method adjusts them to be within a reasonable range, ensuring that no data is lost.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to cap outliers\n",
    "def cap_outliers(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Cap outliers\n",
    "    df_capped = df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "    return df_capped\n",
    "\n",
    "# Cap outliers in numerical features\n",
    "X[numerical_features] = cap_outliers(X[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the boxplot after capping the outliners\n",
    "plt.figure(figsize=(18, 12))\n",
    "X[numerical_features].boxplot()\n",
    "plt.title(\"Boxplot of Numerical Features (After Capping Outliers)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Checking for Class Imbalance\n",
    "\n",
    "## Code:\n",
    "\n",
    "```python\n",
    "# Checking for class imbalance\n",
    "class_distribution = df[\"Target\"].value_counts(normalize=True) * 100\n",
    "print(class_distribution)\n",
    "```\n",
    "\n",
    "## What This Code Does:\n",
    "\n",
    "This code checks whether there is a class imbalance in the target variable (`\"Target\"`) of the dataset. A class imbalance occurs when one class has significantly more data points than the others, which can negatively impact the performance of machine learning models.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Access the Target Column**:\n",
    "   - `df[\"Target\"]` extracts the column named `\"Target\"` from the DataFrame `df`. This is the variable that the model is trying to predict.\n",
    "\n",
    "2. **Calculate Class Distribution**:\n",
    "   - `value_counts(normalize=True)` counts the occurrences of each unique value in the `\"Target\"` column and normalizes the counts to show percentages instead of raw counts.\n",
    "   - Multiplying by `100` converts the proportions into percentages.\n",
    "\n",
    "3. **Print the Distribution**:\n",
    "   - `print(class_distribution)` displays the percentage of data points belonging to each class in the `\"Target\"` column.\n",
    "\n",
    "### Why Check for Class Imbalance?\n",
    "\n",
    "- **Model Bias**: Machine learning models can become biased towards the majority class if the dataset is imbalanced. For example, if 90% of the data points belong to class A and only 10% to class B, the model may learn to predict class A for most cases and ignore class B.\n",
    "- **Performance Issues**: An imbalanced dataset may result in misleading performance metrics (e.g., high accuracy but poor precision/recall for the minority class).\n",
    "- **Next Steps**: If a class imbalance is detected, techniques like oversampling (e.g., SMOTE) or undersampling can be applied to balance the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for class imbalance\n",
    "class_distribution = df[\"Target\"].value_counts(normalize=True) * 100\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Handling Class Imbalance with SMOTE\n",
    "\n",
    "## Code:\n",
    "\n",
    "```python\n",
    "# Handling class imbalance before splitting the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Display the class distribution after applying SMOTE\n",
    "y_resampled_distribution = pd.Series(y_resampled).value_counts(normalize=True) * 100\n",
    "print(y_resampled_distribution)\n",
    "```\n",
    "\n",
    "## What This Code Does:\n",
    "\n",
    "This code uses **SMOTE (Synthetic Minority Oversampling Technique)** to address the class imbalance in the dataset. SMOTE is a technique that generates synthetic samples for the minority class to balance the dataset. The code ensures that the target variable (`y`) has an equal or near-equal number of instances for each class.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Import SMOTE**:\n",
    "   - The code imports `SMOTE` from the `imblearn` library, which specializes in handling imbalanced datasets.\n",
    "\n",
    "2. **Initialize SMOTE**:\n",
    "   - `smote = SMOTE(random_state=42)` initializes the SMOTE object. The `random_state` parameter ensures reproducibility by setting a fixed random seed.\n",
    "\n",
    "3. **Apply SMOTE**:\n",
    "   - `smote.fit_resample(X, y)` oversamples the minority class by creating synthetic data points.\n",
    "   - This results in a new dataset with balanced classes:\n",
    "     - `X_resampled`: The new feature set with synthetic samples added.\n",
    "     - `y_resampled`: The new target variable with balanced classes.\n",
    "\n",
    "4. **Check Class Distribution**:\n",
    "   - `pd.Series(y_resampled).value_counts(normalize=True) * 100` calculates the percentage distribution of each class in the resampled target variable.\n",
    "   - `print(y_resampled_distribution)` displays the class distribution after applying SMOTE.\n",
    "\n",
    "### Why Use SMOTE?\n",
    "\n",
    "- **Balances the Dataset**: SMOTE generates synthetic samples instead of duplicating existing ones, ensuring a better representation of the minority class.\n",
    "- **Improves Model Performance**: A balanced dataset helps machine learning models learn equally well for all classes, improving metrics like precision and recall for the minority class.\n",
    "- **Before Splitting the Data**: SMOTE is applied before splitting the dataset into training and testing sets to ensure the training data is balanced.\n",
    "\n",
    "### Notes:\n",
    "- While SMOTE improves class balance, it should be used carefully, especially for very small datasets, as it introduces synthetic data that may not fully represent the original distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling class imbalance before splitting the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Display the class distribution after applying SMOTE\n",
    "y_resampled_distribution = pd.Series(y_resampled).value_counts(normalize=True) * 100\n",
    "print(y_resampled_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Splitting the Data into Training and Testing Sets\n",
    "\n",
    "## Code:\n",
    "\n",
    "```python\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the training and testing sets \n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "```\n",
    "\n",
    "## What This Code Does:\n",
    "\n",
    "This code splits the resampled dataset into two subsets:\n",
    "1. **Training Set**: Used to train the machine learning model.\n",
    "2. **Testing Set**: Used to evaluate the model's performance on unseen data.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Import the Function**:\n",
    "   - `train_test_split` is imported from `sklearn.model_selection`. This function simplifies the process of dividing data into training and testing subsets.\n",
    "\n",
    "2. **Apply the Split**:\n",
    "   - `train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)` performs the following:\n",
    "     - **Inputs**:\n",
    "       - `X_resampled` and `y_resampled`: The features and target variable after handling class imbalance.\n",
    "       - `test_size=0.2`: Allocates 20% of the data to the testing set and 80% to the training set.\n",
    "       - `random_state=42`: Ensures reproducibility by fixing the random seed.\n",
    "     - **Outputs**:\n",
    "       - `X_train` and `y_train`: The training set (features and target).\n",
    "       - `X_test` and `y_test`: The testing set (features and target).\n",
    "\n",
    "3. **Display the Shapes**:\n",
    "   - `X_train.shape`, `y_train.shape`, `X_test.shape`, and `y_test.shape` show the dimensions of the resulting subsets, confirming the split proportions.\n",
    "\n",
    "### Why Split the Data?\n",
    "\n",
    "- **Train-Test Separation**: Ensures that the model is trained on one subset of the data and evaluated on a completely different subset, preventing overfitting.\n",
    "- **Testing Generalization**: Helps assess how well the model performs on unseen data, which is critical for real-world applications.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the training and testing sets \n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Standardizing Numerical Features\n",
    "\n",
    "## Code:\n",
    "\n",
    "```python\n",
    "# Standardize the numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "```\n",
    "\n",
    "## What This Code Does:\n",
    "\n",
    "This code standardizes the numerical features in the dataset, which means it transforms the data so that each feature has a mean of `0` and a standard deviation of `1`. Standardization is a common preprocessing step in machine learning to ensure that features are on a similar scale, which can improve the performance and convergence of certain models.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Import the StandardScaler**:\n",
    "   - The `StandardScaler` is imported from `sklearn.preprocessing`. It is a tool that standardizes features by removing the mean and scaling to unit variance.\n",
    "\n",
    "2. **Initialize the Scaler**:\n",
    "   - `scaler = StandardScaler()` creates an instance of the scaler.\n",
    "\n",
    "3. **Fit and Transform Training Data**:\n",
    "   - `scaler.fit_transform(X_train[numerical_features])` calculates the mean and standard deviation of each numerical feature in the training data and then standardizes the data using those values.\n",
    "   - This ensures that each feature has:\n",
    "     - Mean = 0\n",
    "     - Standard deviation = 1\n",
    "\n",
    "4. **Transform Testing Data**:\n",
    "   - `scaler.transform(X_test[numerical_features])` uses the mean and standard deviation calculated from the training data to standardize the testing data.\n",
    "   - **Note**: The testing data is only transformed (not fit) to avoid data leakage.\n",
    "\n",
    "5. **Update the Datasets**:\n",
    "   - The standardized values are saved back into `X_train` and `X_test` for the numerical features.\n",
    "\n",
    "### Why Standardize Data?\n",
    "\n",
    "- **Improves Model Performance**: Many machine learning algorithms, such as logistic regression, support vector machines (SVMs), and neural networks, perform better when input features are on a similar scale.\n",
    "- **Speeds Up Training**: Gradient-based optimization algorithms converge faster on standardized data.\n",
    "- **Prevents Bias**: Features with larger ranges can dominate others if not standardized, which may bias the model.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Training and Evaluating the Model\n",
    "\n",
    "## Code:\n",
    "\n",
    "```python\n",
    "# Training the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('+'*50)\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print('+'*50)\n",
    "```\n",
    "\n",
    "## What This Code Does:\n",
    "\n",
    "This code trains a machine learning model using the **Random Forest Classifier** algorithm to classify the data into its target classes. It also evaluates the model's performance using a **classification report** and **accuracy score**.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Import the Random Forest Classifier**:\n",
    "   - `RandomForestClassifier` is imported from `sklearn.ensemble`. This algorithm builds multiple decision trees and combines their outputs for more accurate and stable predictions.\n",
    "\n",
    "2. **Initialize the Model**:\n",
    "   - `rf_classifier = RandomForestClassifier(random_state=42)` creates an instance of the classifier. The `random_state=42` ensures reproducibility of the results by fixing the random seed.\n",
    "\n",
    "3. **Train the Model**:\n",
    "   - `rf_classifier.fit(X_train, y_train)` trains the Random Forest Classifier on the training data (`X_train` and `y_train`). During this process:\n",
    "     - The algorithm creates multiple decision trees.\n",
    "     - Each tree is trained on a subset of the data and features, introducing randomness to make the model robust.\n",
    "\n",
    "4. **Make Predictions**:\n",
    "   - `y_pred = rf_classifier.predict(X_test)` predicts the target variable for the test set (`X_test`) using the trained model.\n",
    "\n",
    "5. **Evaluate the Model**:\n",
    "   - **Classification Report**:\n",
    "     - `classification_report(y_test, y_pred)` generates a detailed report showing the precision, recall, F1-score, and support for each class.\n",
    "       - **Precision**: How many of the predicted positive cases were actually positive.\n",
    "       - **Recall**: How many of the actual positive cases were correctly predicted.\n",
    "       - **F1-score**: A balance between precision and recall.\n",
    "       - **Support**: The number of actual instances for each class.\n",
    "   - **Accuracy Score**:\n",
    "     - `accuracy_score(y_test, y_pred)` calculates the overall accuracy, which is the proportion of correctly classified instances.\n",
    "\n",
    "6. **Print Results**:\n",
    "   - The classification report and accuracy score are printed to provide insights into the model's performance.\n",
    "\n",
    "### Why Use Random Forest?\n",
    "\n",
    "- **Accuracy**: Combines the predictions of multiple decision trees, reducing overfitting and improving accuracy.\n",
    "- **Versatility**: Works well for both classification and regression problems.\n",
    "- **Feature Importance**: Can identify the most important features for prediction.\n",
    "\n",
    "### Notes:\n",
    "- The accuracy score gives an overall measure of performance but should be considered alongside the precision, recall, and F1-score, especially for imbalanced datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('+'*50)\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print('+'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting sensitive features as categorical features\n",
    "# remove loan purpose and education level from sensitive features\n",
    "sensitive_features = [feature for feature in categorical_features if feature not in ['Loan_Purpose', 'Education_Level']]\n",
    "print(sensitive_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just taking 2 sensitive features only that are Employment_Status and Has_Credit_Card\n",
    "slice_sensitive_features = ['Employment_Status', 'Has_Credit_Card']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of AIValidator Help\n",
    "\n",
    "## Code:\n",
    "\n",
    "```python\n",
    "from cimcon.AIValidator import AIValidator as avi\n",
    "\n",
    "# Create an object of the class\n",
    "ai_obj = avi()\n",
    "\n",
    "# Display the available tests and functions\n",
    "ai_obj.aiv_help()\n",
    "```\n",
    "\n",
    "## What This Code Does:\n",
    "\n",
    "This code imports the `AIValidator` class from the `cimcon` library, initializes an instance of the class, and displays a list of supported tests and functions. It also provides guidance on how to access details for specific topics or functionalities.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Import the AIValidator Class**:\n",
    "   - `from cimcon.AIValidator import AIValidator as avi` imports the `AIValidator` class, which is renamed to `avi` for convenience.\n",
    "\n",
    "2. **Initialize the AIValidator Object**:\n",
    "   - `ai_obj = avi()` creates an instance of the `AIValidator` class. This object (`ai_obj`) is used to interact with the library and access its functions.\n",
    "\n",
    "3. **Display Help Information**:\n",
    "   - `ai_obj.aiv_help()` displays:\n",
    "     - **Available Tests**: A list of tests that can be run, such as `Fairness`, `Interpretability`, and `LLMRiskAssessment`.\n",
    "     - **Available Functions**: Methods provided by the class, such as `run_test`, `auth_config`, and `save_result`.\n",
    "   - The output also shows how to get detailed information about any specific test or function:\n",
    "     ```python\n",
    "     ai_obj.aiv_help('<TopicName>')\n",
    "     ```\n",
    "\n",
    "### Example Output:\n",
    "\n",
    "The output includes:\n",
    "\n",
    "- **Tests**:\n",
    "  - `Fairness`: Tests related to fairness in AI models.\n",
    "  - `Interpretability`: Tests related to explaining model decisions.\n",
    "  - `DataDriftAndDataQuality`: Checks for data drift and quality.\n",
    "  - `LLMHallucination`: Assesses hallucinations in Large Language Models (LLMs).\n",
    "\n",
    "- **Functions**:\n",
    "  - `run_test`: Executes a specific test.\n",
    "  - `auth_config`: Configures authentication settings.\n",
    "  - `add_euc`: Adds EUC (End User Computing) details.\n",
    "  - `save_result`: Saves the test results.\n",
    "\n",
    "### Why Use `aiv_help()`?\n",
    "\n",
    "- **Overview**: Quickly provides an overview of the library’s capabilities.\n",
    "- **Guidance**: Explains how to access more detailed information for each test or function.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "To explore a specific test or function, use the `aiv_help()` method with the topic name. For example:\n",
    "```python\n",
    "ai_obj.aiv_help('Fairness')\n",
    "ai_obj.aiv_help('run_test')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cimcon.AIValidator import AIValidator as avi\n",
    "# Create an object of the class\n",
    "ai_obj = avi()\n",
    "ai_obj.aiv_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Running a Fairness Test Using AIValidator\n",
    "\n",
    "## Code:\n",
    "\n",
    "```python\n",
    "params = {'X_test': X_test,\n",
    "          'y_test': y_test,\n",
    "          'y_pred': y_pred,\n",
    "          'sensitive_feature': slice_sensitive_features,\n",
    "          }\n",
    "\n",
    "ai_obj.run_test(\"Fairness\", **params)\n",
    "```\n",
    "\n",
    "## What This Code Does:\n",
    "\n",
    "This code uses the `AIValidator` library to run a **Fairness Test** on the model's predictions. It evaluates how fair the model's predictions are with respect to one or more sensitive features.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Define Parameters**:\n",
    "   - A dictionary named `params` is created to hold the required inputs for the fairness test:\n",
    "     - `X_test`: The test set features used for prediction.\n",
    "     - `y_test`: The actual target values for the test set.\n",
    "     - `y_pred`: The predicted target values generated by the model.\n",
    "     - `sensitive_feature`: A specific feature or a slice of features that are considered sensitive (e.g., gender, race).\n",
    "\n",
    "2. **Run the Fairness Test**:\n",
    "   - `ai_obj.run_test(\"Fairness\", **params)` executes the fairness test:\n",
    "     - `\"Fairness\"`: Specifies the type of test to run.\n",
    "     - `**params`: Passes the parameters defined in the `params` dictionary as arguments to the function.\n",
    "\n",
    "3. **What Happens in the Fairness Test**:\n",
    "   - The test evaluates how the model's predictions vary based on the values of the sensitive feature(s).\n",
    "   - It may compute fairness metrics like:\n",
    "     - **Disparate Impact**: Measures how predictions differ between groups.\n",
    "     - **Equal Opportunity**: Compares true positive rates across groups.\n",
    "     - **Demographic Parity**: Ensures predictions are independent of sensitive attributes.\n",
    "\n",
    "### Why Run a Fairness Test?\n",
    "\n",
    "- **Bias Detection**: Identifies whether the model is biased against certain groups based on sensitive features.\n",
    "- **Compliance**: Ensures the model adheres to ethical standards and regulations.\n",
    "- **Model Improvement**: Provides insights to refine the model for fairer predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'X_test': X_test,\n",
    "          'y_test': y_test,\n",
    "          'y_pred': y_pred,\n",
    "          'sensitive_feature': slice_sensitive_features,\n",
    "          }\n",
    "\n",
    "ai_obj.run_test(\"Fairness\", **params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cimcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
